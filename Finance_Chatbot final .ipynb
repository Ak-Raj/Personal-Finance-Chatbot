{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaPnVfC8ym4b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes torch fastapi uvicorn pyngrok nest-asyncio langchain-ibm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# Load secrets from Colab\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN')\n",
        "    print(\"Credentials loaded successfully from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading credentials: {e}. Please check your Colab Secrets.\")"
      ],
      "metadata": {
        "id": "qIVGyQv5zMgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading IBM Granite model... (This may take a few minutes)\")\n",
        "\n",
        "# Model path from Hugging Face\n",
        "model_path = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "\n",
        "# Configuration to save memory\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, token=hf_token)\n",
        "\n",
        "# Load the Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    token=hf_token,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"✅ IBM Granite Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "-uMuL2PRzRji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# --- IBM Granite tho pani chese main function ---\n",
        "def get_financial_advice(question, persona):\n",
        "    chat_template = [\n",
        "        {\"role\": \"user\", \"content\": f\"You are a helpful Personal Finance assistant. A '{persona}' is asking a question. Answer clearly and concisely. Question: {question}\"}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=250)\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    final_answer = response_text.split(\"<|assistant|>\")[-1].strip()\n",
        "    return final_answer\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# FastAPI app setup\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class Query(BaseModel):\n",
        "    question: str\n",
        "    persona: str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask_ai(query: Query):\n",
        "    advice = get_financial_advice(query.question, query.persona)\n",
        "    return {\"answer\": advice}\n",
        "\n",
        "# Ngrok setup and Server Start\n",
        "ngrok.set_auth_token(ngrok_authtoken)\n",
        "nest_asyncio.apply()\n",
        "http_tunnel = ngrok.connect(8000)\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(f\"✅ IBM Granite Backend Ready! Ee Public URL ni vaadukondi: {http_tunnel.public_url}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "I5Ms9K4Bzbq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVgOxmsC0KAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}