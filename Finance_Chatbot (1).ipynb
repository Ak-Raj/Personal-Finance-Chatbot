{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZ2sqX5ovoe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcdCs1oo0ThT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"Token loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzeA69DT08TS"
      },
      "outputs": [],
      "source": [
        "# Ee cell ni run cheyandi\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# Model ni memory lo takkuva size lo load cheyadaniki ee configuration\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# Tokenizer (mana text ni model ki ardham ayyela marche di)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "# Model (ide mana AI brain)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically GPU ni use chesukuntundi\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k6Qjtks1Ik9"
      },
      "outputs": [],
      "source": [
        "# Ee cell ni run cheyandi\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Secret nunchi token ni load chestunnam\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"Token loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI9eKayk2uwW"
      },
      "outputs": [],
      "source": [
        "# Ee cell ni run cheyandi\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# Model ni memory lo takkuva size lo load cheyadaniki ee configuration\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# Tokenizer (mana text ni model ki ardham ayyela marche di)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "# Model (ide mana AI brain)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically GPU ni use chesukuntundi\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5JfdDTY20Lp"
      },
      "outputs": [],
      "source": [
        "# Ee cell ni run cheyandi\n",
        "def get_financial_advice(question, persona):\n",
        "    # Model ki question ni ela adagalo cheppe format (prompt)\n",
        "    chat_template = [\n",
        "        {\"role\": \"user\", \"content\": f\"You are a helpful Personal Finance assistant. A '{persona}' is asking a question. Answer clearly and concisely. Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    # Template ni model ki ardham ayye format lo convert chestunnam\n",
        "    prompt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Model ki input prepare chestunnam\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Model nunchi answer generate chestunnam\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=250)\n",
        "\n",
        "    # Vachina answer ni manaki ardham ayye text la decode chestunnam\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Only answer part ni teeskuntunnam\n",
        "    final_answer = response_text.split(\"model\\n\")[-1]\n",
        "\n",
        "    return final_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVA2AaKm4jJJ"
      },
      "outputs": [],
      "source": [
        "# Ee cell ni run cheyandi (Shift + Enter)\n",
        "!pip install transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Secret nunchi token ni load chestunnam\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"Token loaded successfully!\")"
      ],
      "metadata": {
        "id": "N0XAQx5O_YoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# Model ni memory lo takkuva size lo load cheyadaniki ee configuration\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# Tokenizer (mana text ni model ki ardham ayyela marche di)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "# Model (ide mana AI brain)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically GPU ni use chesukuntundi\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "_mOegYEn_jjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "def get_financial_advice(question, persona):\n",
        "    # Model ki question ni ela adagalo cheppe format (prompt)\n",
        "    chat_template = [\n",
        "        {\"role\": \"user\", \"content\": f\"You are a helpful Personal Finance assistant. A '{persona}' is asking a question. Answer clearly and concisely. Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    # Template ni model ki ardham ayye format lo convert chestunnam\n",
        "    prompt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Model ki input prepare chestunnam\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Model nunchi answer generate chestunnam\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=250)\n",
        "\n",
        "    # Vachina answer ni manaki ardham ayye text la decode chestunnam\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Only answer part ni teeskuntunnam\n",
        "    final_answer = response_text.split(\"model\\n\")[-1]\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "jjCH2D-E_r_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "# Ikkada mee question and persona ni change chesukondi\n",
        "user_question = \"How can I start investing my money with a small budget?\"\n",
        "user_persona = \"Student\" # \"Student\" or \"Professional\"\n",
        "\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Mana function ni call chesi, AI nunchi answer testunnam\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "\n",
        "# Final answer ni print chestunnam\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "lJCox7ixAcic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "# Ikkada mee question and persona ni change chesukondi\n",
        "user_question = \"How can I start investing my money with a small budget?\"\n",
        "user_persona = \"Professional\" # \"Student\" or \"Professional\"\n",
        "\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Mana function ni call chesi, AI nunchi answer testunnam\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "\n",
        "# Final answer ni print chestunnam\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "RtGxAQGBAnS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VH7BPRhhLuw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee cell ni run cheyandi\n",
        "# Ikkada mee question and persona ni change chesukondi\n",
        "user_question = \"How can I save while repaying student loan?\"\n",
        "user_persona = \"Student\" # \"Student\" or \"Professional\"\n",
        "\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Mana function ni call chesi, AI nunchi answer testunnam\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "\n",
        "# Final answer ni print chestunnam\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "2s4hqN7JAzqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_question = \"help me with financial concerns such as over spending?\"\n",
        "user_persona = \"Student\" # \"Student\" or \"Professional\"\n",
        "\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Mana function ni call chesi, AI nunchi answer testunnam\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "\n",
        "\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "Y0WRYl9fBGYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"How can I save while repaying  loan?\"\n",
        "user_persona = \"Professional\" # \"Student\" or \"Professional\"\n",
        "\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Mana function ni call chesi, AI nunchi answer testunnam\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "\n",
        "# Final answer ni print chestunnam\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "iTce3oAMBcbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Ikkada mee question marchandi\n",
        "user_question = \"how much should Isave from my salary?\"\n",
        "\n",
        "# 2. Ikkada persona marchandi\n",
        "user_persona = \"Professional\"\n",
        "\n",
        "# 3. Ee cell ni run cheyandi\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "WHWj79hEG3UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ikkada mee question marchandi\n",
        "user_question = \"how much should Isave from my salary?\"\n",
        "\n",
        "# 2. Ikkada persona marchandi\n",
        "user_persona = \"Professional\"\n",
        "\n",
        "# 3. Ee cell ni run cheyandi\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "cNcnOFwwHmCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ikkada mee question marchandi\n",
        "user_question = \"how much should I save from my savings?\"\n",
        "\n",
        "# 2. Ikkada persona marchandi\n",
        "user_persona = \"student\"\n",
        "\n",
        "# 3. Ee cell ni run cheyandi\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "t4_RKtU1IGvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ikkada mee question marchandi\n",
        "user_question = \"how much should Isave from my salary?\"\n",
        "\n",
        "# 2. Ikkada persona marchandi\n",
        "user_persona = \"Professional\"\n",
        "\n",
        "# 3. Ee cell ni run cheyandi\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "8yDhxYGnKXWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ee FINAL CELL lo ee విధంగా ivvandi\n",
        "\n",
        "# 1. Ikkada mee complete question ni pettandi\n",
        "# IMPORTANT: Ee triple quotes (\"\"\") chala mukhyam\n",
        "user_question = \"\"\"\n",
        "I am a Professional. Based on my financial situation below, please tell me how much I should save from my salary and give me some advice.\n",
        "\n",
        "- My monthly take-home pay is: 80,000 INR\n",
        "- My monthly expenses are: 45,000 INR (Rent: 20,000, Groceries: 10,000, Transport: 5,000, Other bills: 10,000)\n",
        "- My financial goals are: To save for a down payment for a house in 3 years.\n",
        "- My current debts: I have a personal loan with a monthly EMI of 5,000 INR.\n",
        "- Dependents: I have no dependents.\n",
        "- My current savings rate is very low, around 5,000 INR per month.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Ikkada persona marchandi\n",
        "user_persona = \"Professional\"\n",
        "\n",
        "# 3. Ee cell ni run cheyandi\n",
        "print(f\"Asking AI as a {user_persona}...\")\n",
        "print(\"-\" * 20)\n",
        "advice = get_financial_advice(user_question, user_persona)\n",
        "print(advice)"
      ],
      "metadata": {
        "id": "UcavTarCNPRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byqyP1PZOZK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}